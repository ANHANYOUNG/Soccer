{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3db2d0b-9fb7-4e6a-a2ae-040006a4779d",
   "metadata": {
    "id": "d3db2d0b-9fb7-4e6a-a2ae-040006a4779d"
   },
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e82bf68-4f23-4a14-b69c-b34c351ffd06",
   "metadata": {
    "executionInfo": {
     "elapsed": 3369,
     "status": "ok",
     "timestamp": 1766391961315,
     "user": {
      "displayName": "안한영",
      "userId": "07109810963183759514"
     },
     "user_tz": -540
    },
    "id": "5e82bf68-4f23-4a14-b69c-b34c351ffd06"
   },
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# deep learning framework\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence # padding to handle variable length sequences\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73493fd5-d0b6-4bb5-8ec2-148d5a920a81",
   "metadata": {
    "id": "73493fd5-d0b6-4bb5-8ec2-148d5a920a81"
   },
   "source": [
    "## 2. 하이퍼파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2481df09-f3ac-4bf6-b307-cc24faf5e65c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1766391962647,
     "user": {
      "displayName": "안한영",
      "userId": "07109810963183759514"
     },
     "user_tz": -540
    },
    "id": "2481df09-f3ac-4bf6-b307-cc24faf5e65c",
    "outputId": "5dd1d499-96c6-45df-fedd-fca2a5d1d237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR = 1e-3 # learning rate\n",
    "HIDDEN_DIM = 64 # memorizing capacity\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6953d-8cc2-424d-b9b1-d8b909c508fd",
   "metadata": {
    "id": "08a6953d-8cc2-424d-b9b1-d8b909c508fd"
   },
   "source": [
    "## 3. 데이터 로드 및 전처리\n",
    "### - 에피소드 별 (x,y) 시퀀스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5767755-4f5c-400f-a5d9-ce0d28912a88",
   "metadata": {
    "id": "d5767755-4f5c-400f-a5d9-ce0d28912a88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15435/15435 [00:01<00:00, 11342.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드 수 :  15428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "# arrange by episode and time\n",
    "df = df.sort_values([\"game_episode\", \"time_seconds\"]).reset_index(drop=True)\n",
    "\n",
    "episodes = []\n",
    "targets = []\n",
    "\n",
    "# categorize by game_episode rows\n",
    "# ex) g1 = game_episode 1 data, g2 = game_episode 2 data, ...\n",
    "for _, g in tqdm(df.groupby(\"game_episode\")):\n",
    "    g = g.reset_index(drop=True)\n",
    "    if len(g) < 2: # skip if less than 2 rows\n",
    "        continue\n",
    "\n",
    "    # 정규화된 좌표 준비\n",
    "    # sizing based on standard soccer field dimensions (105m x 68m)\n",
    "    sx = g[\"start_x\"].values / 105.0\n",
    "    sy = g[\"start_y\"].values / 68.0\n",
    "    ex = g[\"end_x\"].values   / 105.0\n",
    "    ey = g[\"end_y\"].values   / 68.0\n",
    "\n",
    "    # number of passes\n",
    "    coords = []\n",
    "    for i in range(len(g)):\n",
    "        # 항상 start는 들어감\n",
    "        coords.append([sx[i], sy[i]])\n",
    "        # 마지막 행 이전까지만 end를 넣음 (마지막 end는 타깃이므로)\n",
    "        if i < len(g) - 1:\n",
    "            coords.append([ex[i], ey[i]])\n",
    "\n",
    "    # [T, 2] # if game_episode has 4 rows -> seq has 7 rows (except last end)\n",
    "    # dim = 2 (x, y)\n",
    "    seq = np.array(coords, dtype=\"float32\")\n",
    "    target = np.array([ex[-1], ey[-1]], dtype=\"float32\")  # 마지막 행 end_x, end_y as target(prediction)\n",
    "\n",
    "    episodes.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "print(\"에피소드 수 : \", len(episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66140869-2d3c-460d-8678-ae6d23d97b88",
   "metadata": {
    "id": "66140869-2d3c-460d-8678-ae6d23d97b88"
   },
   "source": [
    "## 4. Custom Dataset / DataLoader 정의 및 Validation 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1b1d3-851b-4d29-8130-a14410345661",
   "metadata": {
    "id": "c6b1b1d3-851b-4d29-8130-a14410345661",
    "outputId": "1ac9bd9b-3a85-4d37-8cc5-e2c72282b1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train episodes: 12342 valid episodes: 3086\n"
     ]
    }
   ],
   "source": [
    "class EpisodeDataset(Dataset):\n",
    "    # initialize with episodes and targets\n",
    "    def __init__(self, episodes, targets):\n",
    "        self.episodes = episodes\n",
    "        self.targets = targets\n",
    "\n",
    "    # return number of episodes\n",
    "    def __len__(self):\n",
    "        return len(self.episodes)\n",
    "\n",
    "    # data -> tensor\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.episodes[idx])   # [T, 2]\n",
    "        tgt = torch.tensor(self.targets[idx])    # [2]\n",
    "        length = seq.size(0)\n",
    "        return seq, length, tgt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, lengths, tgts = zip(*batch)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    # used to make same length for variable length due to pass counts by padding\n",
    "    # standard: longest sequence in the batch\n",
    "    # short sequences are padded with zeros\n",
    "    # shape: [B, T, 2] = [batch_size, max_seq_length, 2]\n",
    "    padded = pad_sequence(seqs, batch_first=True)\n",
    "\n",
    "    tgts = torch.stack(tgts, dim=0)                # [B, 2]\n",
    "\n",
    "    # to let model know which part is padding\n",
    "    return padded, lengths, tgts\n",
    "\n",
    "# 에피소드 단위 train / valid split\n",
    "# every episode has answer\n",
    "# 8: train the model\n",
    "# 2: validate the model performance on unseen data\n",
    "idx_train, idx_valid = train_test_split(\n",
    "    np.arange(len(episodes)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "episodes_train = [episodes[i] for i in idx_train]\n",
    "targets_train  = [targets[i]  for i in idx_train]\n",
    "episodes_valid = [episodes[i] for i in idx_valid]\n",
    "targets_valid  = [targets[i]  for i in idx_valid]\n",
    "\n",
    "# for training, with shuffle\n",
    "train_loader = DataLoader(\n",
    "    EpisodeDataset(episodes_train, targets_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# for evaluation, no shuffle\n",
    "valid_loader = DataLoader(\n",
    "    EpisodeDataset(episodes_valid, targets_valid),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"train episodes:\", len(episodes_train), \"valid episodes:\", len(episodes_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4f89e-808e-41eb-bbc3-db4a32fab6a8",
   "metadata": {
    "id": "a0d4f89e-808e-41eb-bbc3-db4a32fab6a8"
   },
   "source": [
    "## 5. LSTM 베이스라인 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef323dcf-0a45-4323-ad8e-5bc8ed6c50f1",
   "metadata": {
    "id": "ef323dcf-0a45-4323-ad8e-5bc8ed6c50f1"
   },
   "outputs": [],
   "source": [
    "# use only (x, y) features\n",
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # LSTM: mermorizing\n",
    "        # [input_dim: 2 (x, y), hidden_dim: 64] -> memorize (x, y)'s features by 64 numbers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # FC: by LSTM's 64 info predict (x, y)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)  # (x_norm, y_norm)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, T, 2], lengths: [B]\n",
    "        # ignore padding parts\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h_last = h_n[-1]      # [B, H] 마지막 layer의 hidden state which is the last compressed info\n",
    "        out = self.fc(h_last) # [B, 2]\n",
    "        return out\n",
    "\n",
    "model = LSTMBaseline(input_dim=2, hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
    "criterion = nn.MSELoss() # avg[(Predicted - Target)^2] -> when error is large, loss is large\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR) # gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fca4d7-cd45-4140-bc42-deb004be7976",
   "metadata": {
    "id": "f3fca4d7-cd45-4140-bc42-deb004be7976"
   },
   "source": [
    "## 6. 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a302ea7-6c5b-4f39-b24d-e1403aa091db",
   "metadata": {
    "id": "7a302ea7-6c5b-4f39-b24d-e1403aa091db",
    "outputId": "51c8ed22-b8ac-4f20-ea86-365a1440970e"
   },
   "outputs": [],
   "source": [
    "best_dist = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X, lengths, y in tqdm(train_loader):\n",
    "        X, lengths, y = X.to(DEVICE), lengths.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        pred = model(X, lengths) # forward pass\n",
    "        loss = criterion(pred, y) # compute loss\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update parameters\n",
    "\n",
    "        total_loss += loss.item() * X.size(0) # sum of losses\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset) # average train loss\n",
    "\n",
    "    # --- Valid: 평균 유클리드 거리 ---\n",
    "    # when eval, no gradient calculation\n",
    "    model.eval()\n",
    "    dists = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, lengths, y in tqdm(valid_loader):\n",
    "            X, lengths, y = X.to(DEVICE), lengths.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(X, lengths)\n",
    "\n",
    "            pred_np = pred.cpu().numpy()\n",
    "            true_np = y.cpu().numpy()\n",
    "\n",
    "            pred_x = pred_np[:, 0] * 105.0\n",
    "            pred_y = pred_np[:, 1] * 68.0\n",
    "            true_x = true_np[:, 0] * 105.0\n",
    "            true_y = true_np[:, 1] * 68.0\n",
    "\n",
    "            # calculate Euclidean distance\n",
    "            dist = np.sqrt((pred_x - true_x) ** 2 + (pred_y - true_y) ** 2)\n",
    "            dists.append(dist)\n",
    "\n",
    "    # for eval, use mean distance for easy comparison in human readable way\n",
    "    mean_dist = np.concatenate(dists).mean()  # 평균 유클리드 거리\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"train_loss={train_loss:.4f} | \"\n",
    "        f\"valid_mean_dist={mean_dist:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ----- BEST MODEL 업데이트 -----\n",
    "    if mean_dist < best_dist:\n",
    "        best_dist = mean_dist\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\" --> Best model updated! (dist={best_dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ddd96-c48d-432e-88ac-3872549e9857",
   "metadata": {
    "id": "1d2ddd96-c48d-432e-88ac-3872549e9857"
   },
   "source": [
    "## 7. 평가 데이터셋 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6849ef5-efac-4cfa-9ead-73b1f4dc1760",
   "metadata": {
    "id": "f6849ef5-efac-4cfa-9ead-73b1f4dc1760",
    "outputId": "e45b143d-fa03-40b4-d30c-269b64ade5e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2414 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './test/153363/153363_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m preds_x, preds_y = [], []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(submission.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(submission)):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     g = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.sort_values(\u001b[33m\"\u001b[39m\u001b[33mtime_seconds\u001b[39m\u001b[33m\"\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# 정규화된 좌표 준비\u001b[39;00m\n\u001b[32m     18\u001b[39m     sx = g[\u001b[33m\"\u001b[39m\u001b[33mstart_x\u001b[39m\u001b[33m\"\u001b[39m].values / \u001b[32m105.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UOS\\OneDrive\\바탕 화면\\AN\\CVLAB\\03 축구 해커톤\\open_track1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UOS\\OneDrive\\바탕 화면\\AN\\CVLAB\\03 축구 해커톤\\open_track1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UOS\\OneDrive\\바탕 화면\\AN\\CVLAB\\03 축구 해커톤\\open_track1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UOS\\OneDrive\\바탕 화면\\AN\\CVLAB\\03 축구 해커톤\\open_track1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UOS\\OneDrive\\바탕 화면\\AN\\CVLAB\\03 축구 해커톤\\open_track1\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './test/153363/153363_1.csv'"
     ]
    }
   ],
   "source": [
    "# Best Model Load\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "# exam\n",
    "test_meta = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# answer\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "submission = submission.merge(test_meta, on=\"game_episode\", how=\"left\")\n",
    "\n",
    "preds_x, preds_y = [], []\n",
    "\n",
    "for _, row in tqdm(submission.iterrows(), total=len(submission)):\n",
    "    g = pd.read_csv(row[\"path\"]).sort_values(\"time_seconds\").reset_index(drop=True)\n",
    "    # 정규화된 좌표 준비\n",
    "    sx = g[\"start_x\"].values / 105.0\n",
    "    sy = g[\"start_y\"].values / 68.0\n",
    "    ex = g[\"end_x\"].values / 105.0\n",
    "    ey = g[\"end_y\"].values / 68.0\n",
    "\n",
    "    coords = []\n",
    "    for i in range(len(g)):\n",
    "        # start는 항상 존재하므로 그대로 사용\n",
    "        coords.append([sx[i], sy[i]])\n",
    "        # 마지막 행은 end_x가 NaN이므로 자동으로 제외됨\n",
    "        if i < len(g) - 1:\n",
    "            coords.append([ex[i], ey[i]])\n",
    "\n",
    "    seq = np.array(coords, dtype=\"float32\")  # [T, 2]\n",
    "\n",
    "    x = torch.tensor(seq).unsqueeze(0).to(DEVICE)      # [1, T, 2]\n",
    "    length = torch.tensor([seq.shape[0]]).to(DEVICE)   # [1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x, length).cpu().numpy()[0]       # [2], 정규화 좌표\n",
    "\n",
    "    # resize back to original scale\n",
    "    preds_x.append(pred[0] * 105.0)\n",
    "    preds_y.append(pred[1] * 68.0)\n",
    "print(\"Inference Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69009d4f-ed79-44e0-8ecf-8fd4e2cd0dd3",
   "metadata": {
    "id": "69009d4f-ed79-44e0-8ecf-8fd4e2cd0dd3"
   },
   "source": [
    "## 8. 제출 Submission 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14136f9d-5632-40c3-97ab-0d948dee0948",
   "metadata": {
    "id": "14136f9d-5632-40c3-97ab-0d948dee0948",
    "outputId": "e049b78e-3b8e-4de9-ce25-0446fd963448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: baseline_submit_0.csv\n"
     ]
    }
   ],
   "source": [
    "# submission[\"end_x\"] = preds_x\n",
    "# submission[\"end_y\"] = preds_y\n",
    "# submission[[\"game_episode\", \"end_x\", \"end_y\"]].to_csv(\"./baseline_submit.csv\", index=False)\n",
    "# print(\"Saved: baseline_submit.csv\")\n",
    "\n",
    "import os\n",
    "\n",
    "submission[\"end_x\"] = preds_x\n",
    "submission[\"end_y\"] = preds_y\n",
    "\n",
    "base = \"baseline_submit\"\n",
    "i = 0\n",
    "while os.path.exists(f\"{base}_{i}.csv\"):\n",
    "    i += 1\n",
    "\n",
    "out_path = f\"{base}_{i}.csv\"\n",
    "submission[[\"game_episode\", \"end_x\", \"end_y\"]].to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
