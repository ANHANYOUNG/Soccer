{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3db2d0b-9fb7-4e6a-a2ae-040006a4779d",
   "metadata": {
    "id": "d3db2d0b-9fb7-4e6a-a2ae-040006a4779d"
   },
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e82bf68-4f23-4a14-b69c-b34c351ffd06",
   "metadata": {
    "executionInfo": {
     "elapsed": 3369,
     "status": "ok",
     "timestamp": 1766391961315,
     "user": {
      "displayName": "안한영",
      "userId": "07109810963183759514"
     },
     "user_tz": -540
    },
    "id": "5e82bf68-4f23-4a14-b69c-b34c351ffd06"
   },
   "outputs": [],
   "source": [
    "# file/path uilities\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# for data manipulation/math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# encoding (type_name to number) / split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# deep learning framework\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence # padding to handle variable length sequences\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73493fd5-d0b6-4bb5-8ec2-148d5a920a81",
   "metadata": {
    "id": "73493fd5-d0b6-4bb5-8ec2-148d5a920a81"
   },
   "source": [
    "## 2. 하이퍼파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481df09-f3ac-4bf6-b307-cc24faf5e65c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1766391962647,
     "user": {
      "displayName": "안한영",
      "userId": "07109810963183759514"
     },
     "user_tz": -540
    },
    "id": "2481df09-f3ac-4bf6-b307-cc24faf5e65c",
    "outputId": "5dd1d499-96c6-45df-fedd-fca2a5d1d237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#--- hyperparameter ---\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# cross-validation\n",
    "N_SPLITS = 5 # number of folds\n",
    "FOLD = 0 # which fold for validation\n",
    "\n",
    "# sequence length\n",
    "K = 50 # number of events to consider before the target event if smaller than K, pad with zeros\n",
    "MIN_EVENTS = 2\n",
    "\n",
    "# training parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# model parameters\n",
    "HIDDEN_SIZE = 256 # LSTM hidden size\n",
    "NUM_LAYERS = 2 # number of LSTM layers\n",
    "DROPOUT = 0.1 \n",
    "\n",
    "# data loader parameters\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6953d-8cc2-424d-b9b1-d8b909c508fd",
   "metadata": {
    "id": "08a6953d-8cc2-424d-b9b1-d8b909c508fd"
   },
   "source": [
    "## 3. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5767755-4f5c-400f-a5d9-ce0d28912a88",
   "metadata": {
    "id": "d5767755-4f5c-400f-a5d9-ce0d28912a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num episodes: 15428\n",
      "example cont shape: (49, 9) | example target: [97.13403 41.79307]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "# sort events inside each episode by time, then action_id\n",
    "# action_id is used for duplicate time_seconds\n",
    "df = df.sort_values([\"game_episode\", \"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "# fill missing category text\n",
    "df[\"type_name\"] = df[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "df[\"result_name\"] = df[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "# change category text to idx(number)\n",
    "# mapping number is just name, no matter with performance\n",
    "le_type = LabelEncoder()\n",
    "le_res  = LabelEncoder()\n",
    "df[\"type_id\"] = le_type.fit_transform(df[\"type_name\"])\n",
    "df[\"res_id\"]  = le_res.fit_transform(df[\"result_name\"])\n",
    "\n",
    "# helper for goal geometry (fixed by problem: attacking to the right, goal center at (105,34))\n",
    "GOAL_X, GOAL_Y = 105.0, 34.0\n",
    "\n",
    "episodes = []\n",
    "targets  = []\n",
    "episode_keys = []\n",
    "episode_game_ids = []\n",
    "\n",
    "# build sequences per game_episode\n",
    "for key, g in df.groupby(\"game_episode\"):\n",
    "    g = g.reset_index(drop=True)\n",
    "    if len(g) < 2:\n",
    "        continue\n",
    "\n",
    "    # target is the last event's end point (always pass in your checks)\n",
    "    tx, ty = float(g.loc[len(g)-1, \"end_x\"]), float(g.loc[len(g)-1, \"end_y\"])\n",
    "    if np.isnan(tx) or np.isnan(ty):\n",
    "        continue\n",
    "\n",
    "    # compute dt inside episode\n",
    "    t = g[\"time_seconds\"].astype(\"float32\").values\n",
    "    dt = np.zeros_like(t, dtype=\"float32\")\n",
    "    dt[1:] = t[1:] - t[:-1]\n",
    "    dt[dt < 0] = 0.0  # time-reversal safe-guard\n",
    "\n",
    "    # base coords\n",
    "    sx = g[\"start_x\"].astype(\"float32\").values\n",
    "    sy = g[\"start_y\"].astype(\"float32\").values\n",
    "    ex = g[\"end_x\"].astype(\"float32\").values\n",
    "    ey = g[\"end_y\"].astype(\"float32\").values\n",
    "\n",
    "    # leak-safe masking for last event's end\n",
    "    is_end = np.ones(len(g), dtype=\"float32\")\n",
    "    is_end[-1] = 0.0\n",
    "    ex_mask = ex.copy()\n",
    "    ey_mask = ey.copy()\n",
    "    ex_mask[-1] = 0.0\n",
    "    ey_mask[-1] = 0.0\n",
    "\n",
    "    # is_start is always 1 in event-token\n",
    "    is_start = np.ones(len(g), dtype=\"float32\")\n",
    "\n",
    "    # goal geometry from start position\n",
    "    dxg = GOAL_X - sx\n",
    "    dyg = GOAL_Y - sy\n",
    "    dist_to_goal  = np.sqrt(dxg**2 + dyg**2).astype(\"float32\")\n",
    "    angle_to_goal = np.arctan2(dyg, dxg).astype(\"float32\")  # radians\n",
    "\n",
    "    # categorical ids per event\n",
    "    type_id = g[\"type_id\"].astype(\"int64\").values\n",
    "    res_id  = g[\"res_id\"].astype(\"int64\").values\n",
    "\n",
    "    # continuous features per event (T, F_cont)\n",
    "    # x,y -> start_x,start_y\n",
    "    # end_x,end_y -> masked for last event\n",
    "    # dt -> time gap\n",
    "    # is_start,is_end -> flags\n",
    "    # dist_to_goal, angle_to_goal -> geometry\n",
    "    cont = np.stack(\n",
    "        [sx, sy, ex_mask, ey_mask, dt, is_start, is_end, dist_to_goal, angle_to_goal],\n",
    "        axis=1\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    episodes.append({\n",
    "        \"cont\": cont,         # (T, 9)\n",
    "        \"type_id\": type_id,   # (T,)\n",
    "        \"res_id\": res_id      # (T,)\n",
    "    })\n",
    "    targets.append(np.array([tx, ty], dtype=\"float32\"))\n",
    "    episode_keys.append(key)\n",
    "    episode_game_ids.append(int(str(key).split(\"_\")[0]))\n",
    "\n",
    "print(\"num episodes:\", len(episodes))\n",
    "print(\"example cont shape:\", episodes[0][\"cont\"].shape, \"| example target:\", targets[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66140869-2d3c-460d-8678-ae6d23d97b88",
   "metadata": {
    "id": "66140869-2d3c-460d-8678-ae6d23d97b88"
   },
   "source": [
    "## 4. Custom Dataset / DataLoader 정의 및 Validation 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6b1b1d3-851b-4d29-8130-a14410345661",
   "metadata": {
    "id": "c6b1b1d3-851b-4d29-8130-a14410345661",
    "outputId": "1ac9bd9b-3a85-4d37-8cc5-e2c72282b1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train episodes: 12320 | valid episodes: 3108\n",
      "batch cont_pad: (256, 173, 9)\n",
      "batch type_pad: (256, 173) batch res_pad: (256, 173)\n",
      "lengths: (256,) y: (256, 2)\n",
      "example key: 126357_49\n"
     ]
    }
   ],
   "source": [
    "# build dataset with padding collate for variable-length episodes\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    # store sequences and targets\n",
    "    def __init__(self, episodes, targets, keys):\n",
    "        # episodes: list of dict(cont, type_id, res_id)\n",
    "        self.episodes = episodes\n",
    "        self.targets = targets\n",
    "        self.keys = keys\n",
    "\n",
    "    # number of episodes\n",
    "    def __len__(self):\n",
    "        return len(self.episodes)\n",
    "\n",
    "    # return one episode\n",
    "    def __getitem__(self, idx):\n",
    "        ep = self.episodes[idx]\n",
    "        cont = torch.from_numpy(ep[\"cont\"])                 # (T, F)\n",
    "        type_id = torch.from_numpy(ep[\"type_id\"])           # (T,)\n",
    "        res_id  = torch.from_numpy(ep[\"res_id\"])            # (T,)\n",
    "        y = torch.from_numpy(self.targets[idx])             # (2,)\n",
    "        key = self.keys[idx]\n",
    "        return cont, type_id, res_id, y, key\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # unpack batch\n",
    "    conts, type_ids, res_ids, ys, keys = zip(*batch)\n",
    "\n",
    "    # lengths for packing\n",
    "    lengths = torch.tensor([c.shape[0] for c in conts], dtype=torch.long)\n",
    "\n",
    "    # pad to max length in batch\n",
    "    cont_pad = pad_sequence(conts, batch_first=True, padding_value=0.0)       # (B, T, F)\n",
    "    type_pad = pad_sequence(type_ids, batch_first=True, padding_value=0)      # (B, T)\n",
    "    res_pad  = pad_sequence(res_ids,  batch_first=True, padding_value=0)      # (B, T)\n",
    "    y = torch.stack(ys, dim=0).float()                                        # (B, 2)\n",
    "\n",
    "    return cont_pad.float(), type_pad.long(), res_pad.long(), lengths, y, keys\n",
    "\n",
    "# group split by game_id\n",
    "episode_game_ids = np.array(episode_game_ids, dtype=np.int64)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "tr_idx, va_idx = next(gkf.split(np.zeros(len(episodes)), np.zeros(len(episodes)), groups=episode_game_ids))\n",
    "\n",
    "train_eps = [episodes[i] for i in tr_idx]\n",
    "train_tg  = [targets[i]  for i in tr_idx]\n",
    "train_keys= [episode_keys[i] for i in tr_idx]\n",
    "\n",
    "valid_eps = [episodes[i] for i in va_idx]\n",
    "valid_tg  = [targets[i]  for i in va_idx]\n",
    "valid_keys= [episode_keys[i] for i in va_idx]\n",
    "\n",
    "train_ds = EpisodeDataset(train_eps, train_tg, train_keys)\n",
    "valid_ds = EpisodeDataset(valid_eps, valid_tg, valid_keys)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,  collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"train episodes:\", len(train_ds), \"| valid episodes:\", len(valid_ds))\n",
    "\n",
    "cont_pad, type_pad, res_pad, lengths, y, keys = next(iter(train_loader))\n",
    "print(\"batch cont_pad:\", tuple(cont_pad.shape))\n",
    "print(\"batch type_pad:\", tuple(type_pad.shape), \"batch res_pad:\", tuple(res_pad.shape))\n",
    "print(\"lengths:\", tuple(lengths.shape), \"y:\", tuple(y.shape))\n",
    "print(\"example key:\", keys[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4f89e-808e-41eb-bbc3-db4a32fab6a8",
   "metadata": {
    "id": "a0d4f89e-808e-41eb-bbc3-db4a32fab6a8"
   },
   "source": [
    "## 5. LSTM 베이스라인 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef323dcf-0a45-4323-ad8e-5bc8ed6c50f1",
   "metadata": {
    "id": "ef323dcf-0a45-4323-ad8e-5bc8ed6c50f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "n_type: 26 n_res: 9 cont_dim: 9\n"
     ]
    }
   ],
   "source": [
    "# define lstm model with categorical embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class PassLSTM(nn.Module):\n",
    "    # lstm for sequence regression\n",
    "    def __init__(self, cont_dim, n_type, n_res, emb_dim=16, hidden=256, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # embeddings for categories\n",
    "        self.type_emb = nn.Embedding(n_type, emb_dim)\n",
    "        self.res_emb  = nn.Embedding(n_res,  emb_dim)\n",
    "\n",
    "        in_dim = cont_dim + emb_dim + emb_dim\n",
    "\n",
    "        # lstm backbone\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # regression head -> (x,y)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_pad, type_pad, res_pad, lengths):\n",
    "        # embed categories\n",
    "        te = self.type_emb(type_pad)  # (B,T,emb)\n",
    "        re = self.res_emb(res_pad)    # (B,T,emb)\n",
    "\n",
    "        # concat all features\n",
    "        x = torch.cat([cont_pad, te, re], dim=-1)  # (B,T,in_dim)\n",
    "\n",
    "        # pack padded sequence to ignore padding steps\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "\n",
    "        # use last layer hidden state\n",
    "        h_last = h_n[-1]  # (B, hidden)\n",
    "        out = self.head(h_last)  # (B,2)\n",
    "        return out\n",
    "\n",
    "# sizes for embeddings\n",
    "n_type = int(df[\"type_id\"].max() + 1)\n",
    "n_res  = int(df[\"res_id\"].max() + 1)\n",
    "cont_dim = 9\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PassLSTM(cont_dim=cont_dim, n_type=n_type, n_res=n_res, emb_dim=16, hidden=256).to(device)\n",
    "\n",
    "print(\"device:\", device)\n",
    "print(\"n_type:\", n_type, \"n_res:\", n_res, \"cont_dim:\", cont_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fca4d7-cd45-4140-bc42-deb004be7976",
   "metadata": {
    "id": "f3fca4d7-cd45-4140-bc42-deb004be7976"
   },
   "source": [
    "## 6. 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a302ea7-6c5b-4f39-b24d-e1403aa091db",
   "metadata": {
    "id": "7a302ea7-6c5b-4f39-b24d-e1403aa091db",
    "outputId": "51c8ed22-b8ac-4f20-ea86-365a1440970e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss=32.2651 | valid_mean_dist=30.6791\n",
      "[epoch 2] train_loss=16.7121 | valid_mean_dist=26.0290\n",
      "[epoch 3] train_loss=14.5704 | valid_mean_dist=20.5087\n",
      "[epoch 4] train_loss=10.5141 | valid_mean_dist=16.4103\n",
      "[epoch 5] train_loss=9.5484 | valid_mean_dist=15.7581\n",
      "[epoch 6] train_loss=9.2841 | valid_mean_dist=15.6826\n",
      "[epoch 7] train_loss=9.0207 | valid_mean_dist=15.5961\n",
      "[epoch 8] train_loss=8.9537 | valid_mean_dist=15.0756\n",
      "[epoch 9] train_loss=8.8639 | valid_mean_dist=15.0737\n",
      "[epoch 10] train_loss=8.7824 | valid_mean_dist=14.8192\n",
      "[epoch 11] train_loss=8.6867 | valid_mean_dist=14.9198\n",
      "[epoch 12] train_loss=8.5941 | valid_mean_dist=14.6976\n",
      "[epoch 13] train_loss=8.5559 | valid_mean_dist=15.0847\n",
      "[epoch 14] train_loss=8.5913 | valid_mean_dist=14.7909\n",
      "[epoch 15] train_loss=8.4692 | valid_mean_dist=14.6760\n",
      "[epoch 16] train_loss=8.3902 | valid_mean_dist=14.7133\n",
      "[epoch 17] train_loss=8.3885 | valid_mean_dist=14.6581\n",
      "[epoch 18] train_loss=8.4596 | valid_mean_dist=14.3820\n",
      "[epoch 19] train_loss=8.3486 | valid_mean_dist=14.5454\n",
      "[epoch 20] train_loss=8.3171 | valid_mean_dist=14.3071\n",
      "[epoch 21] train_loss=8.3238 | valid_mean_dist=14.3539\n",
      "[epoch 22] train_loss=8.2976 | valid_mean_dist=14.3929\n",
      "[epoch 23] train_loss=8.2669 | valid_mean_dist=14.2556\n",
      "[epoch 24] train_loss=8.1631 | valid_mean_dist=14.6097\n",
      "[epoch 25] train_loss=8.1606 | valid_mean_dist=14.2334\n",
      "[epoch 26] train_loss=8.1978 | valid_mean_dist=14.2199\n",
      "[epoch 27] train_loss=8.1618 | valid_mean_dist=14.3956\n",
      "[epoch 28] train_loss=8.1896 | valid_mean_dist=14.1566\n",
      "[epoch 29] train_loss=8.0903 | valid_mean_dist=14.2443\n",
      "[epoch 30] train_loss=8.0922 | valid_mean_dist=14.2871\n",
      "best valid_mean_dist: 14.15662090594952\n"
     ]
    }
   ],
   "source": [
    "# train loop with euclidean metric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def mean_euclidean(pred, true):\n",
    "    # compute mean euclidean distance\n",
    "    d = torch.sqrt(((pred - true) ** 2).sum(dim=1))\n",
    "    return d.mean().item()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.SmoothL1Loss()  # stable for regression\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "best_val = 1e9\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # train\n",
    "    model.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for cont_pad, type_pad, res_pad, lengths, y, keys in train_loader:\n",
    "        cont_pad = cont_pad.to(device)\n",
    "        type_pad = type_pad.to(device)\n",
    "        res_pad  = res_pad.to(device)\n",
    "        lengths  = lengths.to(device)\n",
    "        y        = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(cont_pad, type_pad, res_pad, lengths)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    val_dists = []\n",
    "    with torch.no_grad():\n",
    "        for cont_pad, type_pad, res_pad, lengths, y, keys in valid_loader:\n",
    "            cont_pad = cont_pad.to(device)\n",
    "            type_pad = type_pad.to(device)\n",
    "            res_pad  = res_pad.to(device)\n",
    "            lengths  = lengths.to(device)\n",
    "            y        = y.to(device)\n",
    "\n",
    "            pred = model(cont_pad, type_pad, res_pad, lengths)\n",
    "            val_dists.append(mean_euclidean(pred, y))\n",
    "\n",
    "    tr_loss = float(np.mean(tr_losses))\n",
    "    val_dist = float(np.mean(val_dists))\n",
    "\n",
    "    print(f\"[epoch {epoch}] train_loss={tr_loss:.4f} | valid_mean_dist={val_dist:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if val_dist < best_val:\n",
    "        best_val = val_dist\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "print(\"best valid_mean_dist:\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ddd96-c48d-432e-88ac-3872549e9857",
   "metadata": {
    "id": "1d2ddd96-c48d-432e-88ac-3872549e9857"
   },
   "source": [
    "## 7. 평가 데이터셋 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6849ef5-efac-4cfa-9ead-73b1f4dc1760",
   "metadata": {
    "id": "f6849ef5-efac-4cfa-9ead-73b1f4dc1760",
    "outputId": "e45b143d-fa03-40b4-d30c-269b64ade5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference done: 2414\n"
     ]
    }
   ],
   "source": [
    "# 7) inference on test episodes listed in test.csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# paths (edit if your notebook location changes)\n",
    "TEST_META_PATH = \"../data/test.csv\"\n",
    "SUBMISSION_PATH = \"../data/sample_submission.csv\"\n",
    "DATA_ROOT = \"../data\"  # base folder that contains ./test/...\n",
    "\n",
    "# constants for goal geometry\n",
    "GOAL_X = 105.0\n",
    "GOAL_Y = 34.0\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_meta = pd.read_csv(TEST_META_PATH)\n",
    "submission = pd.read_csv(SUBMISSION_PATH)\n",
    "\n",
    "# build one episode features using the same preprocessing as training\n",
    "def build_episode_from_df(g):\n",
    "    # sort inside episode\n",
    "    g = g.sort_values([\"time_seconds\", \"action_id\"]).reset_index(drop=True)\n",
    "\n",
    "    # fill categories\n",
    "    g[\"type_name\"] = g[\"type_name\"].fillna(\"__NA_TYPE__\")\n",
    "    g[\"result_name\"] = g[\"result_name\"].fillna(\"__NA_RES__\")\n",
    "\n",
    "    # handle unseen labels safely\n",
    "    g.loc[~g[\"type_name\"].isin(le_type.classes_), \"type_name\"] = \"__NA_TYPE__\"\n",
    "    g.loc[~g[\"result_name\"].isin(le_res.classes_), \"result_name\"] = \"__NA_RES__\"\n",
    "\n",
    "    type_id = le_type.transform(g[\"type_name\"]).astype(\"int64\")\n",
    "    res_id  = le_res.transform(g[\"result_name\"]).astype(\"int64\")\n",
    "\n",
    "    # time delta (dt)\n",
    "    t = g[\"time_seconds\"].astype(\"float32\").values\n",
    "    dt = np.zeros_like(t, dtype=\"float32\")\n",
    "    dt[1:] = t[1:] - t[:-1]\n",
    "    dt[dt < 0] = 0.0\n",
    "\n",
    "    # coordinates\n",
    "    sx = g[\"start_x\"].astype(\"float32\").values\n",
    "    sy = g[\"start_y\"].astype(\"float32\").values\n",
    "    ex = g[\"end_x\"].astype(\"float32\").values\n",
    "    ey = g[\"end_y\"].astype(\"float32\").values\n",
    "\n",
    "    # replace nan for safety\n",
    "    sx = np.nan_to_num(sx, nan=0.0)\n",
    "    sy = np.nan_to_num(sy, nan=0.0)\n",
    "    ex = np.nan_to_num(ex, nan=0.0)\n",
    "    ey = np.nan_to_num(ey, nan=0.0)\n",
    "\n",
    "    # mask last end (target leakage prevention consistency)\n",
    "    is_start = np.ones(len(g), dtype=\"float32\")\n",
    "    is_end = np.ones(len(g), dtype=\"float32\")\n",
    "    is_end[-1] = 0.0\n",
    "\n",
    "    ex_mask = ex.copy()\n",
    "    ey_mask = ey.copy()\n",
    "    ex_mask[-1] = 0.0\n",
    "    ey_mask[-1] = 0.0\n",
    "\n",
    "    # geometry to goal\n",
    "    dxg = GOAL_X - sx\n",
    "    dyg = GOAL_Y - sy\n",
    "    dist_to_goal  = np.sqrt(dxg**2 + dyg**2).astype(\"float32\")\n",
    "    angle_to_goal = np.arctan2(dyg, dxg).astype(\"float32\")\n",
    "\n",
    "    # continuous features\n",
    "    # cont shape: (T, 9) = [sx, sy, ex_mask, ey_mask, dt, is_start, is_end, dist_to_goal, angle_to_goal]\n",
    "    cont = np.stack(\n",
    "        [sx, sy, ex_mask, ey_mask, dt, is_start, is_end, dist_to_goal, angle_to_goal],\n",
    "        axis=1\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    return cont, type_id, res_id\n",
    "\n",
    "pred_map = {}  # game_episode -> (pred_x, pred_y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, row in test_meta.iterrows():\n",
    "        game_episode = row[\"game_episode\"]\n",
    "\n",
    "        # test.csv has a column \"path\" like \"./test/153363/153363_1.csv\"\n",
    "        rel_path = str(row[\"path\"])\n",
    "        rel_path = rel_path[2:] if rel_path.startswith(\"./\") else rel_path\n",
    "        full_path = os.path.join(DATA_ROOT, rel_path)\n",
    "\n",
    "        g = pd.read_csv(full_path)\n",
    "\n",
    "        cont, type_id, res_id = build_episode_from_df(g)\n",
    "\n",
    "        # to torch tensor and batchify (B=1)\n",
    "        cont_t = torch.from_numpy(cont).unsqueeze(0).to(device)     # (1,T,F)\n",
    "        type_t = torch.from_numpy(type_id).unsqueeze(0).to(device)  # (1,T)\n",
    "        res_t  = torch.from_numpy(res_id).unsqueeze(0).to(device)   # (1,T)\n",
    "        lengths = torch.tensor([cont.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "        pred = model(cont_t.float(), type_t.long(), res_t.long(), lengths)  # (1,2)\n",
    "        pred_xy = pred.squeeze(0).detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "        pred_map[game_episode] = pred_xy\n",
    "\n",
    "# align predictions to sample_submission order\n",
    "preds_x = []\n",
    "preds_y = []\n",
    "missing = []\n",
    "\n",
    "for ge in submission[\"game_episode\"].tolist():\n",
    "    if ge not in pred_map:\n",
    "        missing.append(ge)\n",
    "        preds_x.append(0.0)\n",
    "        preds_y.append(0.0)\n",
    "        continue\n",
    "    px, py = pred_map[ge]\n",
    "    preds_x.append(float(px))\n",
    "    preds_y.append(float(py))\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print(\"warning: missing episodes in pred_map:\", len(missing))\n",
    "\n",
    "submission[\"end_x\"] = preds_x\n",
    "submission[\"end_y\"] = preds_y\n",
    "\n",
    "print(\"inference done:\", len(submission))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f1b0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: LSTM_1_submit_1.csv\n"
     ]
    }
   ],
   "source": [
    "# 8) save submission with auto-increment filename\n",
    "import os\n",
    "\n",
    "base = \"LSTM_1_submit\"\n",
    "ext = \".csv\"\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    out_name = f\"{base}_{i}{ext}\"\n",
    "    if not os.path.exists(out_name):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "submission[[\"game_episode\", \"end_x\", \"end_y\"]].to_csv(out_name, index=False)\n",
    "print(\"saved:\", out_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69009d4f-ed79-44e0-8ecf-8fd4e2cd0dd3",
   "metadata": {
    "id": "69009d4f-ed79-44e0-8ecf-8fd4e2cd0dd3"
   },
   "source": [
    "## 8. 제출 Submission 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14136f9d-5632-40c3-97ab-0d948dee0948",
   "metadata": {
    "id": "14136f9d-5632-40c3-97ab-0d948dee0948",
    "outputId": "e049b78e-3b8e-4de9-ce25-0446fd963448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: LSTM_1_submit_0.csv\n"
     ]
    }
   ],
   "source": [
    "# save submission with auto-increment filename\n",
    "import os\n",
    "\n",
    "base = \"LSTM_1_submit\"\n",
    "ext = \".csv\"\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    out_name = f\"{base}_{i}{ext}\"\n",
    "    if not os.path.exists(out_name):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "submission[[\"game_episode\", \"end_x\", \"end_y\"]].to_csv(out_name, index=False)\n",
    "print(\"saved:\", out_name)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
